plugins {
  id 'org.hidetake.ssh' version '1.5.0'

  // required to build scala example
  id 'scala'

  // useful for working on scala example
  id 'eclipse'

  // we need a fat jar
  id "com.github.johnrengelman.shadow" version "1.2.3"
}

// set the dependencies for compiling the groovy script
repositories {
    mavenCentral()
}

dependencies {
    // required to build the spark example
    compile 'org.scala-lang:scala-library:2.10'
    compile 'org.apache.spark:spark-core_2.10:1.6.2'
    compile 'org.apache.spark:spark-streaming_2.10:1.6.2'
    compile 'org.apache.kafka:kafka_2.10:0.9.0.0'
    compile 'org.apache.kafka:kafka-clients:0.9.0.0'
    compile 'org.apache.kafka:kafka-log4j-appender:0.9.0.0'
    compile files('libs/messagehub.login-1.0.0.jar')
}

// get the cluster connection details
Properties props = new Properties()
props.load(new FileInputStream("$projectDir/etc/bi_connection.properties"))

def jsonFile = file('etc/vcap.json')
def mhProps = new groovy.json.JsonSlurper().parseText(jsonFile.text)

// setup the connection details for ssh
remotes {
    bicluster {
       host = props.hostname
       user = props.username
       password = props.password
    }
}

ssh.settings {
    // disable ssh host key verification 
    // do not use this configuration for production environments
    knownHosts = allowAnyHosts
}

shadowJar.archiveName = 'SparkMessageHubScalaYarn-all.jar'

// uncomment the below line to force clean every run
// delete "./build"

task SubmitToYarn {
    dependsOn shadowJar
    
    doLast {
    	ant.checksum file: "${projectDir}/build/libs/${shadowJar.archiveName}", format: "MD5SUM", property: "md5sumValue"
    
    	def localChecksumValue = "${ant.properties["md5sumValue"]}"
    	
        def remoteProjectDir = "movie-ratings-ingest"

        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {

                // initialise kerberos if this is an enterprise cluster
                execute """
                	which kinit 
                	if [[ \$? -eq 0 ]] 
                	then
                	   kinit -k -t ${props.username}.keytab ${props.username}@IBM.COM
                	else
                	   exit 0
                	fi
                """

                def psCmd = "yarn application -list 2>/dev/null | grep 'biginsights.examples.MessageHubConsumer' | awk '{print \$1}'"

                def application_ids = execute psCmd
                if (application_ids.size() > 0) {
                    throw new GradleException("ERROR:  ** Found yarn application already running.  Kill other applications with `./gradlew KillAll` **")
                }
              
                def remoteChecksumValue = ""
                try {  
                    remoteChecksumValue = execute "md5sum ${remoteProjectDir}/${shadowJar.archiveName}"
                }
                catch (Exception e) {
                    // ignore exception
                }

                if ( remoteChecksumValue.contains(localChecksumValue) ) {
				
                    println "Remote file ${shadowJar.archiveName} is up-to-date. Skipping upload"
					
                } else {
				
	                execute "rm -rf ${remoteProjectDir}"
	
	                execute "mkdir ${remoteProjectDir}"
	
	                println "Uploading SparkMessageHubScala-all.jar to cluster - this may take some time"
	
	                // upload spark script and text file to process
	                put from: "${projectDir}/build/libs/SparkMessageHubScalaYarn-all.jar", 
	                    into: "${remoteProjectDir}/SparkMessageHubScalaYarn-all.jar"
	
	                println "Finished Uploading SparkMessageHubScalaYarn-all.jar"
	            }

                def clz = "--class \"biginsights.examples.MessageHubConsumer\""

                // FIXME! we are passing passwords as command line arguments which will be visible
                //        to other users.  We could probably bundle the properties in the jar file
                //        and the app read them from there.

                def kafka_brokers = "${mhProps.kafka_brokers_sasl}".minus('[').minus(']').replaceAll(/\s*/, "")

                def args = "--conf spark.driver.args=\"${kafka_brokers} ${mhProps.user} ${mhProps.password} ${mhProps.api_key} ${mhProps.kafka_rest_url} ${mhProps.topic} ${props.username}\""

                def cmd = "spark-submit ${args} --master yarn --deploy-mode cluster --num-executors 3 --executor-cores 1 --executor-memory 1G ${clz} ${remoteProjectDir}/SparkMessageHubScalaYarn-all.jar > /dev/null 2>&1 &"

                println "Running: ${cmd}"
                execute cmd
                
                execute """hive -e 'drop table if exists movie_ratings;'"""
                execute """hive -e "create external table movie_ratings ( customer_id string, movie_id string, rating double ) row format delimited fields terminated by ',' location '/user/${props.username}/movie-ratings';" """
                execute """hive -e 'SET hive.mapred.supports.subdirectories=true; SET mapred.input.dir.recursive=true; select * from movie_ratings;'"""
            }
        }
    }
}

task PsAll {
    doLast {
        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {
                def cmd = "yarn application -list 2>/dev/null | grep 'biginsights.examples.MessageHubConsumer' || echo 'No yarn examples running with ID biginsights.examples.MessageHubConsumer'"
                execute cmd
            }
        }
    }
}

task KillAll {
    doLast {
        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {

                def cmd = "yarn application -list 2>/dev/null | grep 'biginsights.examples.MessageHubConsumer' | awk '{print \$1}'"

                def application_ids = execute cmd

                if (application_ids.size() > 0) {
                    application_ids.split('\n').each { appId ->
                        try {
                            execute "yarn application -kill ${appId}"
                        } catch (Exception e) {
                            println "Ignoring error from kill command"
                        }
                    }
                }
            }
        }
    }
}

task CatHdfs {
    doLast {
        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {
                execute 'hdfs dfs -cat /user/$USER/movie-ratings/*/*'
            }
        }
    }
}

task HiveSelect {
    doLast {
        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {
                execute """hive -e 'SET hive.mapred.supports.subdirectories=true; SET mapred.input.dir.recursive=true; select * from movie_ratings;'"""
            }
        }
    }
}
